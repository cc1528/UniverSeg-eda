{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/JJGO/UniverSeg\n",
    "!python -m pip install -r ./UniverSeg/requirements.txt"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import math\n",
    "import itertools\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import einops as E\n",
    "from collections import defaultdict\n",
    "import pathlib\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Literal, Optional, Tuple\n",
    "from torch.utils.data import Dataset\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import logging\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import sys\n"
   ],
   "id": "7c55f44ce4cd4df7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "sys.path.append('UniverSeg')\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "from universeg import universeg\n",
    "model = universeg(pretrained=True)\n",
    "_ = model.to(device)"
   ],
   "id": "8c20c608b2298787"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "# Define paths \n",
    "dataset_dir = '/kaggle/input/partitioned-data/partitioned_dataset_original'\n",
    "\n",
    "\n",
    "test_input_folder = os.path.join(dataset_dir, 'images', 'test')\n",
    "test_mask_folder = os.path.join(dataset_dir, 'masks', 'test')\n",
    "\n",
    "support_input_folder = os.path.join(dataset_dir, 'images', 'train')\n",
    "support_mask_folder = os.path.join(dataset_dir, 'masks', 'train')\n",
    "\n",
    "# Print the directories to verify\n",
    "print(\"Test Input Folder:\", test_input_folder)\n",
    "print(\"Test Mask Folder:\", test_mask_folder)\n",
    "print(\"Support Input Folder:\", support_input_folder)\n",
    "print(\"Support Mask Folder:\", support_mask_folder)\n",
    "\n"
   ],
   "id": "857e3f2e7fc32cf3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def process_image(image_path: pathlib.Path, size: Tuple[int, int]):\n",
    "    \"\"\"Process input image with hot encoded selection of areas.\"\"\"\n",
    "    # Load input image\n",
    "    img = PIL.Image.open(image_path)\n",
    "    img = img.resize(size, resample=PIL.Image.BILINEAR)\n",
    "    img = img.convert(\"L\")\n",
    "    img = np.array(img)\n",
    "    img = img.astype(np.float32)\n",
    "    return img\n",
    "\n",
    "\n",
    "def process_seg(path: pathlib.Path, size: Tuple[int, int]) -> np.ndarray:\n",
    "    \"\"\"Process segmentation mask.\"\"\"\n",
    "    seg = PIL.Image.open(path)\n",
    "    seg = seg.resize(size, resample=PIL.Image.NEAREST)\n",
    "    seg = seg.convert(\"L\")\n",
    "    seg = np.array(seg)\n",
    "\n",
    "    # One-hot encoded representation of segmentation mask\n",
    "    seg = np.stack([seg == 0, seg == 150, seg == 76])\n",
    "    seg = seg.astype(np.float32)\n",
    "\n",
    "    return seg\n",
    "\n",
    "\n",
    "def load_dataset(input_folder: str, mask_folder: str, size: Tuple[int, int] = (128, 128)):\n",
    "    \"\"\"Load dataset from input and mask folders.\"\"\"\n",
    "    data = []\n",
    "    input_path = pathlib.Path(input_folder)\n",
    "    mask_path = pathlib.Path(mask_folder)\n",
    "\n",
    "    # Sort images based on numerical values in filenames\n",
    "    input_files = sorted(input_path.glob(\"*.png\"), key=lambda x: int(x.stem.split('_')[-1]))\n",
    "\n",
    "    for file in input_files:\n",
    "        img = process_image(file, size=size)\n",
    "        img_name = file.stem\n",
    "\n",
    "        # Load segmentation mask\n",
    "        seg_file = mask_path / f\"{img_name}_mask.png\"\n",
    "        if seg_file.exists():\n",
    "            seg = process_seg(seg_file, size=size)\n",
    "        else:\n",
    "            print(f\"Mask file '{seg_file}' not found. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        data.append((img / 255.0, seg))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class JNU_FMI(Dataset):\n",
    "    input_folder: str\n",
    "    mask: str\n",
    "    size: Tuple[int, int] = (128, 128)\n",
    "    label: Optional[Literal[\"head\", \"symp\", \"background\"]] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self._data = load_dataset(self.input_folder, self.mask, size=self.size)\n",
    "        T = torch.from_numpy\n",
    "\n",
    "        # Convert to tensors and add channel dimension to images\n",
    "        self._data = [(T(x)[None], T(y)) for x, y in self._data]\n",
    "        \n",
    "        if self.label is not None:\n",
    "            self._ilabel = {\"head\": 1, \"symp\": 2, \"background\": 0}[self.label]\n",
    "        \n",
    "        self.idxs = list(range(len(self._data)))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img, seg = self._data[self.idxs[idx]]\n",
    "        if self.label is not None:\n",
    "            seg = seg[self._ilabel][None]  \n",
    "           # seg = seg[None]  # Add channel dimension back\n",
    "        #print(f\"Image shape: {img.shape}, Segmentation shape: {seg.shape}\")  # Debugging print statement\n",
    "        return img, seg"
   ],
   "id": "d6417fcda1b12b8d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    " # Change to 'symp' or 'head' for other labels\n",
    "label = 'background'\n",
    "d_support = JNU_FMI(input_folder=support_input_folder, mask=support_mask_folder, size=(128, 128), label=label)\n",
    "print(f\"Support set length: {len(d_support)}\")\n",
    "d_test = JNU_FMI(input_folder=test_input_folder, mask=test_mask_folder, size=(128, 128), label=label)\n",
    "print(f\"Test set length: {len(d_test)}\")"
   ],
   "id": "a07fca419c577d0f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def dice_score(y_pred: torch.Tensor, y_true: torch.Tensor) -> float:\n",
    "    y_pred = y_pred.long()\n",
    "    y_true = y_true.long()\n",
    "    intersection = (y_pred * y_true).sum().item()\n",
    "    total = y_pred.sum().item() + y_true.sum().item()\n",
    "    if total == 0:\n",
    "        return 1.0  \n",
    "    score = 2 * intersection / total\n",
    "    return score\n",
    "\n",
    "def accuracy_score(y_pred: torch.Tensor, y_true: torch.Tensor) -> float:\n",
    "    y_pred_bin = (y_pred > 0.5).float()\n",
    "    if y_true.sum().item() == 0 and y_pred_bin.sum().item() == 0:\n",
    "        return 1.0\n",
    "    correct = (y_pred_bin == y_true).float().sum()\n",
    "    total = y_true.numel()\n",
    "    return (correct / total).item()\n",
    "\n",
    "def sensitivity_score(y_pred: torch.Tensor, y_true: torch.Tensor) -> float:\n",
    "    y_pred_bin = (y_pred > 0.5).float()\n",
    "    true_positive = (y_pred_bin * y_true).sum().item()\n",
    "    false_negative = ((1 - y_pred_bin) * y_true).sum().item()\n",
    "    if true_positive + false_negative == 0:\n",
    "        return 1.0\n",
    "    return true_positive / (true_positive + false_negative)\n",
    "\n",
    "def precision_score(y_pred: torch.Tensor, y_true: torch.Tensor) -> float:\n",
    "    y_pred_bin = (y_pred > 0.5).float()\n",
    "    true_positive = (y_pred_bin * y_true).sum().item()\n",
    "    false_positive = (y_pred_bin * (1 - y_true)).sum().item()\n",
    "    if true_positive + false_positive == 0:\n",
    "        return 1.0\n",
    "    return true_positive / (true_positive + false_positive)\n",
    "\n",
    "def jaccard_score(y_pred: torch.Tensor, y_true: torch.Tensor) -> float:\n",
    "    y_pred_bin = (y_pred > 0.5).float()\n",
    "    intersection = (y_pred_bin * y_true).sum().item()\n",
    "    union = y_pred_bin.sum().item() + y_true.sum().item() - intersection\n",
    "    if union == 0:\n",
    "        return 1.0\n",
    "    return intersection / union\n"
   ],
   "id": "77a7d6c705a419e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def inferencesupport(model, image, label, support_images, support_labels, threshold=0.5):\n",
    "    image, label = image.to(device), label.to(device)\n",
    "\n",
    "    # inference\n",
    "    logits = model(\n",
    "        image[None],\n",
    "        support_images[None],\n",
    "        support_labels[None]\n",
    "    )[0]  \n",
    "\n",
    "    soft_pred = torch.sigmoid(logits)\n",
    "    hard_pred = (soft_pred > threshold).float().clip(0, 1)\n",
    "\n",
    "    # score\n",
    "    dicescore = dice_score(hard_pred, label)\n",
    "    accuracy = accuracy_score(hard_pred, label)\n",
    "    sensitivity = sensitivity_score(hard_pred, label)\n",
    "    precision = precision_score(hard_pred, label)\n",
    "    jaccard = jaccard_score(hard_pred, label)\n",
    "\n",
    "    # return a dictionary of all relevant variables\n",
    "    return {\n",
    "        'Image': image,\n",
    "        'Soft Prediction': soft_pred,\n",
    "        'Prediction': hard_pred,\n",
    "        'Ground Truth': label,\n",
    "        'score': dicescore,\n",
    "        'accuracy': accuracy,\n",
    "        'sensitivity': sensitivity,\n",
    "        'precision': precision,\n",
    "        'jaccard': jaccard\n",
    "    }\n"
   ],
   "id": "610b316616d05f34"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "#code for running experiments",
   "id": "3b00d258f7be09cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(filename='experiment.log', level=logging.INFO, format='%(asctime)s %(message)s')\n",
    "\n",
    "# Set global seed for reproducibility\n",
    "global_seed = 42\n",
    "np.random.seed(global_seed)\n",
    "torch.manual_seed(global_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(global_seed)\n",
    "\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def sample_support(seed, support_size):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idxs = rng.integers(0, len(d_support), size=support_size)\n",
    "    support_images, support_labels = zip(*[d_support[i] for i in idxs])\n",
    "    support_images = torch.stack(support_images).to(device)\n",
    "    support_labels = torch.stack(support_labels).to(device)\n",
    "    return support_images, support_labels\n",
    "\n",
    "def get_model_size(model):\n",
    "    torch.save(model.state_dict(), \"temp.pth\")\n",
    "    model_size = os.path.getsize(\"temp.pth\") / (1024 * 1024)  \n",
    "    os.remove(\"temp.pth\")\n",
    "    return model_size\n",
    "\n",
    "def run_experiment(support_size, n_ensemble, threshold=0.5):\n",
    "    # Create empty DataFrames for each metric\n",
    "    df_dicescore = pd.DataFrame(columns=['dice_score', 'support_size', 'ensemble_count', 'threshold'])\n",
    "    df_accuracy = pd.DataFrame(columns=['accuracy'])\n",
    "    df_sensitivity = pd.DataFrame(columns=['sensitivity'])\n",
    "    df_precision = pd.DataFrame(columns=['precision'])\n",
    "    df_jaccard = pd.DataFrame(columns=['jaccard'])\n",
    "\n",
    "    # Get various support sets\n",
    "    seeds = range(n_ensemble)\n",
    "    supports = {\n",
    "        seed: sample_support(seed, support_size)\n",
    "        for seed in range(n_ensemble)\n",
    "    }\n",
    "\n",
    "    # Initialize timing accumulator\n",
    "    inference_times = []\n",
    "\n",
    "    # Go through the number of experiments\n",
    "    for i in tqdm(range(len(d_test)), desc=\"Processing images\"):  # Process the entire dataset\n",
    "        results = defaultdict(list)\n",
    "        for j in range(n_ensemble):\n",
    "            # Set the seed for reproducibility\n",
    "            seed = global_seed + j\n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "            support_images, support_labels = sample_support(seed, support_size)\n",
    "            image, label = d_test[i]\n",
    "\n",
    "            start_time = time()  # Start timing\n",
    "            vals = inferencesupport(model, image, label, support_images, support_labels, threshold)\n",
    "            end_time = time()  # End timing\n",
    "\n",
    "            inference_time = end_time - start_time\n",
    "            inference_times.append(inference_time)\n",
    "\n",
    "            for k, v in vals.items():\n",
    "                results[k].append(v)\n",
    "\n",
    "        # Aggregate results\n",
    "        ensemble = torch.mean(torch.stack(results['Soft Prediction']), dim=0)\n",
    "        results['Soft Prediction'].append(ensemble)\n",
    "        results['Prediction'].append((ensemble > threshold).float())\n",
    "        results['Ground Truth'].append(label)\n",
    "        results['score'].append(dice_score((ensemble > threshold).float(), label.to(device)))\n",
    "        results['jaccard'].append(jaccard_score((ensemble > threshold).float(), label.to(device)))\n",
    "        results['sensitivity'].append(sensitivity_score((ensemble > threshold).float(), label.to(device)))\n",
    "        results['precision'].append(precision_score((ensemble > threshold).float(), label.to(device)))\n",
    "\n",
    "        # Append the metrics to the DataFrames\n",
    "        df_dicescore = pd.concat([df_dicescore, pd.DataFrame({'dice_score': [results['score'][-1]], 'support_size': support_size, 'ensemble_count': n_ensemble, 'threshold': threshold})], ignore_index=True)\n",
    "        if 'accuracy' in results:\n",
    "            df_accuracy = pd.concat([df_accuracy, pd.DataFrame({'accuracy': [results['accuracy'][-1]]})], ignore_index=True)\n",
    "        df_sensitivity = pd.concat([df_sensitivity, pd.DataFrame({'sensitivity': [results['sensitivity'][-1]]})], ignore_index=True)\n",
    "        df_precision = pd.concat([df_precision, pd.DataFrame({'precision': [results['precision'][-1]]})], ignore_index=True)\n",
    "        df_jaccard = pd.concat([df_jaccard, pd.DataFrame({'jaccard': [results['jaccard'][-1]]})], ignore_index=True)\n",
    "\n",
    "    # Compute mean values and save to a single DataFrame\n",
    "    mean_values = {\n",
    "        'dice_score': df_dicescore['dice_score'].mean(),\n",
    "        'accuracy': df_accuracy['accuracy'].mean(),\n",
    "        'sensitivity': df_sensitivity['sensitivity'].mean(),\n",
    "        'precision': df_precision['precision'].mean(),\n",
    "        'jaccard': df_jaccard['jaccard'].mean(),\n",
    "        'support_size': support_size,\n",
    "        'ensemble_count': n_ensemble,\n",
    "        'inference_time': np.mean(inference_times),  # mean inference time\n",
    "        'threshold': threshold\n",
    "    }\n",
    "\n",
    "    mean_df = pd.DataFrame(mean_values, index=[0])\n",
    "\n",
    "    # Calculate throughput\n",
    "    total_images = len(d_test) * n_ensemble\n",
    "    total_time = np.sum(inference_times)\n",
    "    throughput = total_images / total_time\n",
    "\n",
    "    # Add throughput and model size to mean_df\n",
    "    mean_df['throughput'] = throughput\n",
    "    mean_df['model_size'] = get_model_size(model)\n",
    "    \n",
    "    return df_dicescore, df_accuracy, df_sensitivity, df_precision, df_jaccard, mean_df\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'support_size': [3, 5, 10, 14, 20],\n",
    "    'n_ensemble': [5, 8],\n",
    "    'threshold': [0.3]  # use '0.5' or '0.7' to run evaluation with the other thresholds.\n",
    "\n",
    "}\n",
    "\n",
    "# Initialize lists to store results\n",
    "all_dicescores = []\n",
    "all_accuracies = []\n",
    "all_sensitivities = []\n",
    "all_precisions = []\n",
    "all_jaccards = []\n",
    "all_means = []\n",
    "\n",
    "# Run experiments sequentially\n",
    "for params in ParameterGrid(param_grid):\n",
    "    support_size = params['support_size']\n",
    "    n_ensemble = params['n_ensemble']\n",
    "    threshold = params['threshold']\n",
    "    \n",
    "    df_dicescore, df_accuracy, df_sensitivity, df_precision, df_jaccard, mean_df = run_experiment(support_size, n_ensemble, threshold)\n",
    "    \n",
    "    # Save the DataFrames to CSV files\n",
    "    df_dicescore.to_csv(f'dicescore_support_{support_size}_ensemble_{n_ensemble}_threshold_{threshold}.csv', index=False)\n",
    "    df_accuracy.to_csv(f'accuracy_support_{support_size}_ensemble_{n_ensemble}_threshold_{threshold}.csv', index=False)\n",
    "    df_sensitivity.to_csv(f'sensitivity_support_{support_size}_ensemble_{n_ensemble}_threshold_{threshold}.csv', index=False)\n",
    "    df_precision.to_csv(f'precision_support_{support_size}_ensemble_{n_ensemble}_threshold_{threshold}.csv', index=False)\n",
    "    df_jaccard.to_csv(f'jaccard_support_{support_size}_ensemble_{n_ensemble}_threshold_{threshold}.csv', index=False)\n",
    "    mean_df.to_csv(f'mean_metrics_support_{support_size}_ensemble_{n_ensemble}_threshold_{threshold}.csv', index=False)\n",
    "    \n",
    "    # Store results in lists\n",
    "    all_dicescores.append(df_dicescore)\n",
    "    all_accuracies.append(df_accuracy)\n",
    "    all_sensitivities.append(df_sensitivity)\n",
    "    all_precisions.append(df_precision)\n",
    "    all_jaccards.append(df_jaccard)\n",
    "    all_means.append(mean_df)\n",
    "\n",
    "# Combine all means into a single DataFrame\n",
    "combined_means = pd.concat(all_means, ignore_index=True)\n",
    "combined_means.to_csv('combined_mean_metrics_0.3.csv', index=False)\n",
    "\n",
    "# Print combined means\n",
    "print(\"Combined Mean Metrics:\")\n",
    "print(combined_means)\n",
    "\n",
    "\n"
   ],
   "id": "54b24adbd89861a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "#Plots",
   "id": "fb51384cbdab49e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the combined metrics CSV file\n",
    "combined_means = pd.read_csv('/kaggle/working/combined_mean_metrics_0.3.csv')\n",
    "\n",
    "# Set a color palette\n",
    "colors = sns.color_palette(\"husl\", len(combined_means.columns[:-5]))\n",
    "\n",
    "# Function to plot and save the metrics for each threshold\n",
    "def plot_metrics_for_threshold(combined_means, threshold):\n",
    "    # Filter the combined_means DataFrame for the current threshold\n",
    "    threshold_combined_means = combined_means[combined_means['threshold'] == threshold]\n",
    "    \n",
    "    # Plot mean values line plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    x_labels = [f\"({int(row['support_size'])}, {int(row['ensemble_count'])})\" for _, row in threshold_combined_means.iterrows()]\n",
    "    unique_x_labels = sorted(set(x_labels), key=lambda x: (int(float(x.split(\", \")[0][1:])), int(float(x.split(\", \")[1][:-1]))))\n",
    "    \n",
    "    for metric in threshold_combined_means.columns[:-6]:  # Exclude the last five columns ('support_size', 'ensemble_count', 'inference_time', 'throughput', 'model_size')\n",
    "        plt.plot(x_labels, threshold_combined_means[metric], label=metric)\n",
    "\n",
    "    plt.xlabel('Experiment (Support Size, Ensemble Count)')\n",
    "    plt.ylabel('Mean Value')\n",
    "    plt.title(f'Mean Values for Different Metrics (Threshold {threshold})')\n",
    "    plt.xticks(unique_x_labels, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'mean_values_plot_threshold_{threshold}.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot inference time\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(x_labels, threshold_combined_means['inference_time'], marker='o')\n",
    "    plt.xlabel('Experiment (Support Size, Ensemble Count)')\n",
    "    plt.ylabel('Inference Time (s)')\n",
    "    plt.title(f'Inference Time for Different Experiments (Threshold {threshold})')\n",
    "    plt.xticks(unique_x_labels, rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'inference_time_plot_threshold_{threshold}.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot throughput\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(x_labels, threshold_combined_means['throughput'], marker='o')\n",
    "    plt.xlabel('Experiment (Support Size, Ensemble Count)')\n",
    "    plt.ylabel('Throughput (images/s)')\n",
    "    plt.title(f'Throughput for Different Experiments (Threshold {threshold})')\n",
    "    plt.xticks(unique_x_labels, rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'throughput_plot_threshold_{threshold}.png')\n",
    "    plt.show()\n",
    "    \n",
    "        # Plot boxplot for dice scores\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    all_dicescores_combined.boxplot(column='dice_score', by=['support_size', 'ensemble_count'])\n",
    "    plt.xlabel('Experiment (Support Size and Ensemble Count)')\n",
    "    plt.ylabel('Dice Score')\n",
    "    plt.title(f'Boxplot of Dice Scores (Threshold {threshold})')\n",
    "    plt.suptitle('')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/dice_scores_boxplot_threshold_{threshold}.png')\n",
    "    plt.show()\n",
    "\n",
    "# Combine all dice scores into a single DataFrame\n",
    "all_dicescores_combined = pd.concat(all_dicescores, ignore_index=True)\n",
    "\n",
    "\n",
    "threshold_values = [0.3]  #use '0.5' or '0.7' for the other threshold values\n",
    "# Iterate over thresholds and generate plots\n",
    "for threshold in threshold_values:\n",
    "    plot_metrics_for_threshold(combined_means, threshold)\n",
    "\n",
    "print(\"Plots generated and saved with identifiers for the threshold.\")\n"
   ],
   "id": "3ad8f9ae26b8e1ce"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
