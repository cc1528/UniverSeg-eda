{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import os\n",
    "from typing import Tuple, List\n",
    "import random\n",
    "import segmentation_models_pytorch as smp\n",
    "from sklearn.metrics import jaccard_score, f1_score, precision_score, recall_score, accuracy_score\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Augmentations\n",
    "transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=45, p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.ElasticTransform(p=0.5),\n",
    "    A.GridDistortion(p=0.5),\n",
    "    A.GaussNoise(p=0.2),\n",
    "    A.CLAHE(p=0.2),\n",
    "    A.Blur(p=0.2),\n",
    "    A.RandomResizedCrop(height=128, width=128, p=0.5),\n",
    "    A.Normalize(mean=(0.5,), std=(0.5,), max_pixel_value=255.0),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Separate transform for validation/testing (no augmentation)\n",
    "val_transform = A.Compose([\n",
    "    A.Normalize(mean=(0.5,), std=(0.5,), max_pixel_value=255.0),\n",
    "    ToTensorV2()\n",
    "])\n"
   ],
   "id": "968e1867e2c4b1f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir: str, mask_dir: str, transform=None, size: Tuple[int, int] = (128, 128)):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.size = size\n",
    "        self.image_paths = sorted(os.listdir(image_dir), key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "        self.mask_paths = sorted(os.listdir(mask_dir), key=lambda x: int(x.split('_')[-2]))\n",
    "\n",
    "        assert len(self.image_paths) == len(self.mask_paths), \"Number of images and masks should be the same\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.image_paths[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.mask_paths[idx])\n",
    "\n",
    "        image = Image.open(image_path).convert(\"L\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        image = image.resize(self.size, resample=Image.BILINEAR)\n",
    "        mask = mask.resize(self.size, resample=Image.NEAREST)\n",
    "\n",
    "        image = np.array(image)\n",
    "        mask = np.array(mask).astype(np.int64)\n",
    "\n",
    "        background_value = 0\n",
    "        head_value = 150\n",
    "        symp_value = 76\n",
    "\n",
    "        mask = np.where(mask == background_value, 0, mask)\n",
    "        mask = np.where(mask == head_value, 1, mask)\n",
    "        mask = np.where(mask == symp_value, 2, mask)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Function to calculate metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    jaccard = jaccard_score(y_true, y_pred, average='macro')\n",
    "    dice = f1_score(y_true, y_pred, average='macro')\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    sensitivity = recall \n",
    "    \n",
    "    return jaccard, dice, precision, recall, accuracy, sensitivity\n",
    "\n",
    "# Function to evaluate model on validation or test set\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    y_true_all = []\n",
    "    y_pred_all = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]  \n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            y_true_all.extend(labels.cpu().numpy().flatten())\n",
    "            y_pred_all.extend(preds.cpu().numpy().flatten())\n",
    "\n",
    "    y_true_all = np.array(y_true_all)\n",
    "    y_pred_all = np.array(y_pred_all)\n",
    "\n",
    "    return calculate_metrics(y_true_all, y_pred_all)\n",
    "\n"
   ],
   "id": "71d779ce74cd049d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_image_dir = r'C:\\Users\\cinth\\Documentos\\ams\\data_science\\actual_thesis\\codes\\MedSAM_Universeg_2024\\datasets\\data\\dataset_complete_1\\partitioned_dataset_original\\images\\train'\n",
    "train_mask_dir = r'C:\\Users\\cinth\\Documentos\\ams\\data_science\\actual_thesis\\codes\\MedSAM_Universeg_2024\\datasets\\data\\dataset_complete_1\\partitioned_dataset_original\\masks\\train'\n",
    "val_image_dir = r'C:\\Users\\cinth\\Documentos\\ams\\data_science\\actual_thesis\\codes\\MedSAM_Universeg_2024\\datasets\\data\\dataset_complete_1\\partitioned_dataset_original\\images\\val'\n",
    "val_mask_dir = r'C:\\Users\\cinth\\Documentos\\ams\\data_science\\actual_thesis\\codes\\MedSAM_Universeg_2024\\datasets\\data\\dataset_complete_1\\partitioned_dataset_original\\masks\\val'\n",
    "test_image_dir = r'C:\\Users\\cinth\\Documentos\\ams\\data_science\\actual_thesis\\codes\\MedSAM_Universeg_2024\\datasets\\data\\dataset_complete_1\\partitioned_dataset_original\\images\\test'\n",
    "test_mask_dir = r'C:\\Users\\cinth\\Documentos\\ams\\data_science\\actual_thesis\\codes\\MedSAM_Universeg_2024\\datasets\\data\\dataset_complete_1\\partitioned_dataset_original\\masks\\test'\n",
    "\n",
    "train_dataset = SegmentationDataset(train_image_dir, train_mask_dir, transform=transform)\n",
    "val_dataset = SegmentationDataset(val_image_dir, val_mask_dir, transform=val_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "test_dataset = SegmentationDataset(test_image_dir, test_mask_dir, transform=val_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ],
   "id": "706d46e9091182d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Function to train and save multiple models\n",
    "def train_and_save_models(num_models, train_loader, val_loader, device, num_epochs=50):\n",
    "    model_paths = []\n",
    "    for i in range(num_models):\n",
    "        print(f\"Training model {i+1}/{num_models}\")\n",
    "        model = smp.Unet(encoder_name=\"efficientnet-b0\", encoder_weights=\"imagenet\", in_channels=1, classes=3)\n",
    "        model = model.to(device)\n",
    "        \n",
    "        optimizer = optim.Adam(model.parameters(), lr=5e-5, weight_decay=1e-6)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)\n",
    "        weights = torch.tensor([0.5270, 0.8379, 0.98], device=device)\n",
    "        criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "        \n",
    "        best_loss = float('inf')\n",
    "        patience = 5\n",
    "        epochs_without_improvement = 0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Starting epoch {epoch+1}/{num_epochs}\")\n",
    "            model.train()\n",
    "           \n",
    "\n",
    "\n",
    "            running_loss = 0.0\n",
    "            train_y_true_all = []\n",
    "            train_y_pred_all = []\n",
    "\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device).long()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                train_y_true_all.extend(labels.cpu().numpy().flatten())\n",
    "                train_y_pred_all.extend(preds.cpu().numpy().flatten())\n",
    "\n",
    "            epoch_loss = running_loss / len(train_loader)\n",
    "            train_y_true_all = np.array(train_y_true_all)\n",
    "            train_y_pred_all = np.array(train_y_pred_all)\n",
    "            train_metrics = calculate_metrics(train_y_true_all, train_y_pred_all)\n",
    "            train_jaccard, train_dice, train_precision, train_recall, train_accuracy, train_sensitivity = train_metrics\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss}, Jaccard Score: {train_jaccard}, Dice Coefficient: {train_dice}, Precision: {train_precision}, Recall: {train_recall}, Accuracy: {train_accuracy}, Sensitivity: {train_sensitivity}\")\n",
    "\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_y_true_all = []\n",
    "            val_y_pred_all = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_loader:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device).long()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                    preds = torch.argmax(outputs, dim=1)\n",
    "                    val_y_true_all.extend(labels.cpu().numpy().flatten())\n",
    "                    val_y_pred_all.extend(preds.cpu().numpy().flatten())\n",
    "\n",
    "            val_loss /= len(val_loader)\n",
    "            val_y_true_all = np.array(val_y_true_all)\n",
    "            val_y_pred_all = np.array(val_y_pred_all)\n",
    "            val_metrics = calculate_metrics(val_y_true_all, val_y_pred_all)\n",
    "            val_jaccard, val_dice, val_precision, val_recall, val_accuracy, val_sensitivity = val_metrics\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss}, Jaccard Score: {val_jaccard}, Dice Coefficient: {val_dice}, Precision: {val_precision}, Recall: {val_recall}, Accuracy: {val_accuracy}, Sensitivity: {val_sensitivity}\")\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                model_path = f'best_model_{i}.pth'\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                print(f\"Epoch {epoch+1}: New best model saved with validation loss {val_loss}\")\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                print(f\"Epoch {epoch+1}: No improvement in validation loss\")\n",
    "                if epochs_without_improvement >= patience:\n",
    "                    print(\"Early stopping due to no improvement in validation loss\")\n",
    "                    break\n",
    "\n",
    "        model_paths.append(model_path)\n",
    "    return model_paths\n",
    "\n",
    "# Train and save 2 models\n",
    "num_models = 2\n",
    "model_paths = train_and_save_models(num_models, train_loader, val_loader, device)\n",
    "\n",
    "# Load the models\n",
    "models = []\n",
    "for model_path in model_paths:\n",
    "    model = smp.Unet(encoder_name=\"efficientnet-b0\", encoder_weights=None, in_channels=1, classes=3)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "\n",
    "# Function to make ensemble predictions\n",
    "def ensemble_predict(models, dataloader, device):\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            preds = []\n",
    "\n",
    "            for model in models:\n",
    "                outputs = model(inputs)\n",
    "                softmax_outputs = torch.softmax(outputs, dim=1)\n",
    "                preds.append(softmax_outputs)\n",
    "\n",
    "            # Average the softmax outputs from all models\n",
    "            avg_preds = torch.mean(torch.stack(preds), dim=0)\n",
    "            final_preds = torch.argmax(avg_preds, dim=1)\n",
    "\n",
    "            all_preds.extend(final_preds.cpu().numpy())\n",
    "\n",
    "    return np.array(all_preds)\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "def evaluate_ensemble(ensemble_preds, dataloader):\n",
    "    y_true_all = []\n",
    "    for _, labels in dataloader:\n",
    "        y_true_all.extend(labels.numpy().flatten())\n",
    "    y_true_all = np.array(y_true_all)\n",
    "    \n",
    "    return calculate_metrics(y_true_all, ensemble_preds.flatten())\n",
    "\n",
    "# Get ensemble predictions for the test set\n",
    "ensemble_preds = ensemble_predict(models, test_loader, device)\n",
    "\n",
    "# Get test metrics for the ensemble model\n",
    "test_metrics = evaluate_ensemble(ensemble_preds, test_loader)\n",
    "test_jaccard, test_dice, test_precision, test_recall, test_accuracy, test_sensitivity = test_metrics\n",
    "print(f\"Ensemble Test Metrics - Jaccard Score: {test_jaccard}, Dice Coefficient: {test_dice}, Precision: {test_precision}, Recall: {test_recall}, Accuracy: {test_accuracy}, Sensitivity: {test_sensitivity}\")\n",
    "\n",
    "# Save the ensemble test metrics to a file\n",
    "with open('ensemble_test_metrics.txt', 'w') as f:\n",
    "    f.write(f\"Ensemble Test Metrics:\\n\")\n",
    "    f.write(f\"Jaccard Score: {test_jaccard}\\n\")\n",
    "    f.write(f\"Dice Coefficient: {test_dice}\\n\")\n",
    "    f.write(f\"Precision: {test_precision}\\n\")\n",
    "    f.write(f\"Recall: {test_recall}\\n\")\n",
    "    f.write(f\"Accuracy: {test_accuracy}\\n\")\n",
    "    f.write(f\"Sensitivity: {test_sensitivity}\\n\")\n",
    "\n",
    "# Save the final ensemble model weights\n",
    "# Averaging the weights of the models \n",
    "def average_weights(models):\n",
    "    avg_model = models[0]\n",
    "    for key in avg_model.state_dict().keys():\n",
    "        for model in models[1:]:\n",
    "            avg_model.state_dict()[key] += model.state_dict()[key]\n",
    "        avg_model.state_dict()[key] = avg_model.state_dict()[key] / len(models)\n",
    "    return avg_model\n",
    "\n",
    "ensemble_model = average_weights(models)\n",
    "torch.save(ensemble_model.state_dict(), 'ensemble_model_final.pth')\n",
    "print(\"Final ensemble model weights saved as 'ensemble_model_final.pth'\")\n",
    "\n"
   ],
   "id": "ecec169ec8767c60"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
